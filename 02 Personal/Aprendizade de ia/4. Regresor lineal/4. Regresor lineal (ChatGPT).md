
Un regresor lineal es un modelo matemático utilizado en estadísticas y aprendizaje automático para predecir una variable continua (o valor numérico) basándose en una o más variables independientes. La forma básica de un regresor lineal asume que la relación entre las variables independientes $X$ y la variable dependiente $Y$ es lineal, es decir, puede representarse como una línea recta en un espacio bidimensional o un plano en un espacio tridimensional.

La expresión general para un regresor lineal simple con una sola variable independiente es:

$$ Y = b_0 + b_1 \cdot X + \varepsilon $$

- $Y$ es la variable dependiente que queremos predecir.
- $X$  es la variable independiente.
- $b_0$ es la ordenada al origen (intercepto), que representa el valor de \( Y \) cuando \( X \) es igual a cero.
- $b_1$ es la pendiente de la línea, que indica cómo cambia $Y$ con respecto a un cambio unitario en $X$.
- $\varepsilon$ es el término de error, que captura la variabilidad no explicada por la relación lineal.

Cuando hay múltiples variables independientes, se utiliza un regresor lineal múltiple, y la expresión se generaliza a:

$$ Y = b_0 + b_1 \cdot X_1 + b_2 \cdot X_2 + \ldots + b_n \cdot X_n + \varepsilon $$

Donde $X_1, X_2, \ldots, X_n$ son las variables independientes, y $b_0, b_1, b_2, \ldots, b_n$ son los coeficientes que el modelo ajusta durante el proceso de entrenamiento.

El objetivo del entrenamiento de un regresor lineal es encontrar los valores óptimos de los coeficientes $b_0, b_1, \ldots, b_n$ de manera que minimicen la suma de los cuadrados de los errores  $\varepsilon$ entre las predicciones del modelo y los valores reales de la variable dependiente en el conjunto de datos de entrenamiento.

Los regresores lineales son simples y fáciles de interpretar, pero a veces pueden no capturar relaciones más complejas en los datos. En casos donde la relación entre las variables no es lineal, se pueden explorar modelos más avanzados, como regresión polinómica o modelos de regresión no lineal.
